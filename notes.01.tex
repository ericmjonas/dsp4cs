\input{header.tex}

\title{Signals and LTI Systems}
\begin{document}
\maketitle



\section{What is a signal }

A signal is a function of one or more independent variables, such as
$x(t)$, where we generally let $t$ range over $(-\infty, \infty)$. We
often think of such signals as representing some observation of the
world -- the voltage on a wire, the air pressure at a microphone, or
electromagnetic vibrations in the vacuum.

We will work with signals in continuous time, and represent them by
$x(t)$, $t \in \mathbb{R}$. We will also work with signals in discrete
time, $x[n]$, $n \in \mathbb{Z}$. Note that the behavior of these two
different classes of signals can be both subtle and profound, and
transitioning between one and the other implactes some of the most
classic results in signal processing, such as the Shannon/Nyquist
sampling theorem. However, a large number of the properties that apply
to discrete signals apply to continuous-time signals and vice versa,
so we will often state properties of one that apply equally to the
other. 


A great deal of ink has been spilled about which signals are
in some sense admissable, but we will gloss over most of those
issues here. We define the energy in a signal to be

\[
E_\infty = \int_{-\infty}^{\infty} |x(t)|^2 dt
\]

and note that many signals we work with will have infinite energy. 
There are mathematical formalisms you can use to handle all of this, 
but they are beyond the scope of this survey course. 

\subsection{Periodic Signals}
We will often deal with periodic signals, where
\[
x(t) = x(t+T)
\]

The canonical examples being sines and cosines. We will become intimately 
familiar with the periodic complex exponential signal, 
\[
x(t) = e^{j \omega t}
\]
which we remember from Kindergarden can be expressed via Euler's equation 
as 
\[
e^{j \omega_0 t} = \cos \omega_0 t + j \sin \omega_0 t
\]

Note that things become slightly more complex in the discrete case. We can
define
\fxwarning{Add discrete time example}

\subsection{the unit impulse}
The unit impulse, shows up in many areas of electrical
engineering, physics, and probability. We will first consider the 
discrete time version

The discrete-time unit impulse can be used to ``sample'' a signal
at time zero, as
\[
x[n]\delta[n] = x[0]\delta[n]
\]

The Dirac delta function plays a similar role in the continuous-time domain.
\[
\int_{-\infty}^\infty f(x) \delta(x) dx = f(0)
\]

\section{Analytic Signals}
Most signals in the real world are real-valued. 

Physicsists always say ``Well we can just use a complex signal and
then take the real part'' but

1. Why? 

2. Ok, there are a whole bunch of ways to create a complex signal from
a real one, why do we do it a certain way? 

3. I and Q -- quadrature signals

4. Negative Frequency

\section{Systems}

We speak of systems, which take in a signal $x(t)$ and produce a signal $y(t)$. WE
draw them like this. 

Electrical engineers and computer scientists can bond over our desire
to draw boxes and connect them with arrows. It's important to note that
a system can look at as much of the signal as it wants, but it also
must produce an output for every time. 

We could just add stuff together

We could have a little circuit and describe it with an ODE


\subsection{Memory}
A \textit{memoryless} system has no state -- the output $y[n]$ can only be a function
of the current value of the signal $x[n]$. The system 
\[
y[n] = x[n]^2
\]
is an example of a memoryless system, whereas the system which sums all previous
inputs (an accumulator) 
\[
y[n] = \sum_{k=-\infty}^n x[k]
\]
is not. 


\subsection{invertability}

\subsection{causality}
A system is causal if the value of $y(t)$ only depends on the values
of $x(\tau)$, $\tau < t$. Thus all memoryless systems are causal, 
but the accumulator described above is also causal. 


\subsection{ stability }
A system is stable if bounded inputs produce bounded outputs. The accumulator
above is not stable. 

\subsection{Linearity}
This is one of the most important properties a system can have. We're
all familiar with linearity so I'll just state that 
\begin{enumerate}
\item The response to $x_1(t) + x_2(t)$ is $y_1(t) + y_2(t)$
\item The response to $a x(t)$ is $a y(t)$ where $a \in \mathbb{C}$. 
\end{enumerate}

This gives rise to superposition! A ton of real-world systems are linear, 
and will make up the majority of topics in this survey. 

\subsection{Time Invariance}
Time-invariance is another crucial property. Simply stated, a
system is time-invariant if a time-shit in the input
produces an equivalent time-shift in the output. 

If $y(t)$ is the response of a system to an input $x(t)$, the system
is time-invariant if, when fed an input $x(t-t_0)$ its output is $y(t
- t_0)$.
 
Many physical systems are time invariant, and even those that aren't
can often be modeled locally as time-invariant. 
\fxwarning{Give example}


\section{Convolution}


\subsection{Impulse representation of a signal}
We can write any input signal as a sum of weighted impulse
functions. 

For a linear system, if we know the input response to $x[n]$ then we
know the response to $ax[n]$ is just $ay[n]$. We also 
knwo that 

Thus we can represent the output of a system as a
sum of its outputs to a buch of input impulse functions,
scaled appropriately. 







All linear, time-invariant systems can be expressed as the 
convolution of an input signal $x(t)$ with a special function, 
the \textit{impulse reseponse} of that system, $h(t)$

First we will review convolution in the continuous-time
case

\[
y(t) = \int_{-\infty}^{\infty} x(\tau) h(t - \tau) d\tau
\]

and the discrete-time case:
\[
y[n] = \sum_{k=-\infty}^{\infty} x[k] h[n -k]
\]


``Flip and slide'' 

Asymptotic complexity 

\subsection{The linear algebra view}
A Toeplitz matrix has constant diagonals
\[ %\arraycolsep=4pt
 G = 
 \begin{bmatrix*}[r]
    1 \\
    2&1\\
   -1&2&1\\
     &-1&2&1\\
     &&-1&2&1\\
     &&&-1&2&1\\
     &&&&&&\ddots\\
     &&&&&&&\ddots\\
     &&&&&&&&\ddots\\
     &&&&&&&&&1
  \end{bmatrix*}
\]

With sufficient padding, the convolution of a signal $x[n]$ with
$h[n]$ can be represented as multiplication by a toeplitz matrix. 

\end{document}
